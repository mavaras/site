# **Image and Video compression using Deep Learning, a brief literature review**
###### 23/4/2021
---

<p align="center">
  <img src="https://www.winzip.com/static/wz/images/learn/features/compress-folder/compress-folder.jpg" width="70%" style="border-radius: 5%">
</p>

File compression has always been a key research topic in computer science by its direct relationship with software efficiency, general performance and speed. Compress files is a process in which a file is taken and reduced by its size with preserving its content as accurately as possible. This simple precept can lead us to improve the transmission and handling of that file and, consequently, to improve some particular software which uses some file-related tasks like storing, transfering or modifying files. This happens, for example, in the most part of software living on the web.  
Over the years, traditional compression systems and algorithms have been replaced in some cases by Deep Learning solutions. Particularly, for image compression, some models have been developed demonstrating a better performance than classic image codecs like JPEG [1] or its successor JPEG 2000. For video compression, there have been others that were able to overcome other classic standards, e.g., H.264/AVC [2].  
All this effort to develop better compression methods based on Deep Learning answers to the need of make internet faster and most capable to handle the year-to-year increase in traffic, especially on video, being this one the main traffic source of the whole internet, constituting more than three quarters of the total traffic and, on the other hand, to the increasing demand of higher resolutions and *bitrates* on video files to streaming or VOD applications. Based on these two ideas we can make a double statement. First, that researching about new image or video or whatever kind of file compression methods its synonymous to contributing to a more scalable, faster and more efficient internet. Second, related to the first statement, we can say that researching new file comrpession methods it's a task directly related to cualitative improve the final user experience (QoE).  


## Image compression
Images record the visual scene of our natural world and are often compressed for efficient network exchange and local storage.  
We can divide image compression related studies in two groups. First, those early ones that proposed a set of methods to compress files, these being applicable to image compression. Three methods stand out among all: Huffman algorithm, Golomb coding and arithmetic coding. These methods were based on the simple idea of reducing the statistical redundancy in the file using entropy coding. This coding technique is a lossy compression method which is independent of environmental features. Basically, it's about giving priority to those frequent features over the less frequent ones. A few decades later, other methods using transformation coding were proposed. Transformation coding is a type of compression that works finding that components are expendable within the set with none or small risk to reduce the size of the whole set. A smaller set implies easier file storage and transmission. The main objective of this technique is to reduce *bandwith*. Some example studies include mathematical methods like Fourier transform [3], Hadamard transform [4] or Walsh transform [4].  
Some time later, in 1992, an image compression standard emerged that is in force today, JPEG [1]. It's largely based on previously cited studies and became the most popular standard in the following years up to the present. It's a lossy image files compression method that works by splitting the image in blocks or *chunks* and taking them to a DCT representation. In other words, we can say that JPEG works taking the pixels of an image, representing them in a different way and, once at that point, encoding them. In this different representation we have two channels, one for brightness and othe for color. Subsampling is applied to color channels to reduce its dimensionality. This process is inspired in human vision behaviour: our eye is most sensible to brightness changes than to color changes, so that subsampling won't be percieved in detail, if it is the case. Finally, I want to highlight a paper published in 1999 by J. Jian. That was the first study stating that neural networks can be used to make image compression. This publication [5] sat down the foundations on which ways neural networks can positively contribute to image compression, suggesting that those methods can match and even surpass traditional algorithms and standards.  

## Video compression
For videos, we must take into consideration one particularity that images does not have, and it is the temporal component. This adds a fourth dimension to the problem spectre. As the videos are sequences of time sorted images, to try to mitigate that time redundancy in videos have always been the enemy to beat in this kind of video compression tasks. Initially, some traditional compression algorithms arosed like H.264 [2] and H.265 [6], both based on lineal predictive coding. Aswell other methods stood out like MPEG [7] o HEVC [6], both based on pixel reconstruction techniques using nearest neighbors methods. A few years after, a couple of solutions among which stands out by its interest and originality that based on  frame prediction by blocks. On the other hand, within Deep Learning residual frame methods were proposed [8], which try, by using frame substraction, to remove thos parts of the file which could be redundant or exapendable or just not too important in relation to the information that contributed to the whole. Also, mode decision methods [9], consisting of reducing the computational complexity of the coding process, or other alternative methods like entropy coding [10] or postprocessing optimization [11]. This methods were ultimately intended to achieve some improvements in one concrete part of the traditional algorithms  compressing process. The next step to this improvements were to build a whole system that makes the compression process, the so-called *end-to-end* schemes.  
Regarding RNNs, were some few studies among which stands out one carried out by researchers of the University of Texas [12], where it is intended to introduce a frame interpolation component to the compression process, that is, the artificial or semi-artificial intermediate frames generation. Other interesting study is one mentioned before when mentioning residual frame. This studiy were published by Tong Chen et. al. and it has special interest because mixes a number of CNNs and Huffman coding algorithm [13]. It propose a different solution. It is a series of nested convolutional neural networks prior to a coding process where first a scalar quantization intervenes followed by Huffman encoding. This process pretends to code what the author calls fMaps, which are a data structure used to basically represent video features, prioritizing those frequent ones in favor of those which are less frequent, just as Huffman algorithm poses. This study shows that with a non-typical architecture, a similar performance to the H.264 standard could be achieved, putting an interesting alternative to video compression right on the table.  


---
### References
[1] *G. K. Wallace, “The JPEG still Picture Compression Standard,” Architecture, vol. 38, no. 1, 1992.*  
[2] *L. Balaji, “H . 264 / SVC Mode Decision Based on Mode Correlation and Desired Mode List.”*  
[3] *M. H. Rasheed, O. M. Salih, M. M. Siddeq, and M. A. Rodrigues, “Image compression based on 2D Discrete Fourier Transform and matrix minimization algorithm,” Array, vol. 6, no. February, p. 100024, 2020, doi: 10.1016/j.array.2020.100024.*  
[4] *K. T. Win, N. Aye, and A. Y. E. Htwe, “Image Compression Based on Modified Walsh- Hadamard Transform ( Mwht ),” no. 6, pp. 70–74, 2015.*  
[5] *J. Jiang, “Image compression with neural networks - a survey,” Signal Process. Image Commun., vol. 14, no. 9, pp. 737–760, 1999, doi: 10.1016/S0923- 5965(98)00041-1.*  
[6] *G. J. Sullivan, J. R. Ohm, W. J. Han, and T. Wiegand, “Overview of the high efficiency video coding (HEVC) standard,” IEEE Trans. Circuits Syst. Video Technol., vol. 22, no. 12, pp. 1649–1668, 2012, doi: 10.1109/TCSVT.2012.2221191.*  
[7] *C. Simon, M. Torcoli, and J. Paulus, “Mpeg-H audio for improving accessibility in broadcasting and streaming,” arXiv, pp. 1–11, 2019.*  
[8] *T. Chen, H. Liu, Q. Shen, T. Yue, X. Cao, and Z. Ma, “DeepCoder: A deep neural network based video compression,” 2017 IEEE Vis. Commun. Image Process. VCIP 2017, vol. 2018-Janua, pp. 1–4, 2018, doi: 10.1109/VCIP.2017.8305033.*  
[9] *Z. Liu, X. Yu, Y. Gao, S. Chen, X. Ji, and D. Wang, “CU partition mode decision for HEVC hardwired intra encoder using convolution neural network,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5088–5103, 2016, doi: 10.1109/TIP.2016.2601264.*  
[10] *R. Song, D. Liu, H. Li, and F. Wu, “Neural network-based arithmetic coding of intra prediction modes in HEVC,” 2017 IEEE Vis. Commun. Image Process. VCIP 2017, vol. 2018-Janua, pp. 1–4, 2018, doi: 10.1109/VCIP.2017.8305104.*  
[11] *G. Lu, W. Ouyang, D. Xu, X. Zhang, Z. Gao, and M. T. Sun, “Deep kalman filtering network for video compression artifact reduction,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 11218 LNCS, pp. 591–608, 2018, doi: 10.1007/978-3-030-01264-9_35.*  
[12] *C. Y. Wu, N. Singhal, and P. Krähenbühl, “Video compression through image interpolation,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 11212 LNCS, pp. 425–440, 2018, doi: 10.1007/978-3-030-01237-3_26.*  
[13] *T. Chen, H. Liu, Q. Shen, T. Yue, X. Cao, and Z. Ma, “DeepCoder: A deep neural network based video compression,” 2017 IEEE Vis. Commun. Image Process. VCIP 2017, vol. 2018-Janua, pp. 1–4, 2018, doi: 10.1109/VCIP.2017.8305033.*  



