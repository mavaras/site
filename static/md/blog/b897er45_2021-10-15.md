# **Transfer Learning in Deep Learning, at a glance**
###### 15/10/2021
---


Deep Learning is like playing Lego or [Meccano](https://www.amazon.com/-/es/Super-juego-de-construcci%C3%B3n-Meccano/dp/B000GOF5S2); we have several parts or components composed by other smaller simpler parts that you can play with by combining them and building new functional cool structures. We can build a ramshackle ugly car or a hyper helpful crane that rides super fast and never goes down carrying huge weights. That depends on our building skills. So, a Deep Learning model could be seen as a super-piece composed by multiple pieces attached between them in a particular order that works for a particular problem. Then, you can go beyond that and use this super-piece/model to make a superior super-piece/model made by more complex pieces. And so on. You can scale this process according the complexity of your problem to solve and your imagination.  
  
<p align="center">
  <img src="https://img.blogs.es/anexom/wp-content/uploads/2017/11/mecano-meccano-tecnologia-piezas.jpg" width="70%" style="border-radius: 5%">
</p>

Based on this mental schema of what Deep Learning is, we can understand **Transfer Learning** as a technique consisting of taking one of this super-pieces and split it in, for instance, a half, so we can use the output of the last part of the first half. It's like cut the super-piece/model flow in a certain point where we would receive an output that fits our problem.
We should start by having a strong high-performance model at a source task that we can use as the basis of the model that we need to solve our concrete problem. For example, we may have a ResNet50 but no need of using it as a whole piece, we may want to use the 80% of it, or maybe we may have a better performance for our particular problem if we remove the last two layers of the net and use it. That small tricks are what transfer learing basically is. **Transfer Learning** introduces the concept of keep past learning current for new models. The learn of the new problem depends in this cases on the learning of the previously learned tasks, enhancing the model knowledge, having this way a knowledge transition between problems, similar to what we humans usually do internally.

## Why using it?
So, why not creating from scratch a new model for our problem? That probably will puff up our chest and make us feel like a super-engineer, right?  
Well, I'm a true fan of that quote "simplicity is hard". I'd add that, always being realistic, simplicity should be a must. That's why **Transfer Learning**. It saves time and provides a shortcut to achieve the thing that matters the most to engineers (should be): solve problems.
Obviously, as in AI problems should happen, we wouldn't completely know if **Transfer Learning** is worth to be applied in our project if we do not try it and deploy and evaluate our model. Although, there are some cases where we can easily bet on **Transfer Learning** as a method to have in mind. If we are developing a truck recognition system, we can easily obtain from it a car recognition model by applying **Transfer Learning**. Not everything is trial and error in AI, rather intuition. In some cases, we may be having a more robust model that can perform a variety of tasks, fitted to our concrete problem to solve. Having this new model, the main idea behind **Transfer Learning** is that knowledge can be transfered to other future models. 
Also, using **Transfer Learning** in huge production problems will reduce the resources invested in this process, which are usually high, being able to simplify everything.  
We can see the benefits of using **Transfer Learning** in too many problem domains, including natural language processing, audio and video processing, image captioning models, etc.

## Challenges of Transfer Learning
The main idea behind **Transfer Learning** is not new and as mentioned above it can decrease the work to build complex or specifical Deep Learning problems.  
We can start saying that there are more posibilities of being successful doing **Transfer Learning** if we pick not just one source problem, but a set of candidate source problems. That's one challenge we have here. If this is done, next step is to define a system to choose the best one. Usually, useful source problems are simpler as our current specific problem, so an easy way to choose the best source problem, among lots of other complex ways proposed in several papers [1, 2, 3], would be to sort that problems by its complexity or difficulty. For instance, if we have to recognize trucks we easily understand that recognizing cars it's an easier task than recognizing trucks in terms or recognition complexity. Simply, cars is a more general domain than trucks. We even may be including trucks in cars recognition. Idea is to identify one source problem into wich we can fit our desired problem, not neccesarily being a subproblem or a subset of that domain but simply being kind of a more "general" problem.  
<p align="center">
  <img src="https://miro.medium.com/max/1330/1*7Ip2_SeOz_BoruHEytEMlQ.png" width="100%" style="border-radius: 5%">
</p>
<br>
Another different approach to this topic will be to not just choose one single source problem, but a subgroup of them. This can lead to have a better source problem knowledge and decrease the risk of negative transfer. There are some authors that develop solutions in that direction [4, 5].  
Negative transfer indeed is the main issue that **Transfer Learning** currently has. It refers to the reduction of the accuracy of a deep learning model after retraining. This can be caused from too high a dissimilarity of the problem domains or the inability of the model to train for the new domainâ€™s data set (in addition to the new dataset itself). This has led to methods to quantitatively identify similarity of problem domains to understand negative transfer potential (such as choosing a car recognition problem as a source for recognizing trucks).


## Conclusions
**Transfer Learning** is a basic piece that Machine Learning has and currently is on trend by its ability to make Deep Learning models flexible, reusable and scalable.  
To finish this blog entry I want to recommend, if you wanna go further into this topic, to check this brief Andrew Ng tutorial named [*Nuts and bolts of building AI applications using Deep Learning*](https://media.nips.cc/Conferences/2016/Slides/6203-Slides.pdf) which was used in his remarkable presentation at NIPS 2016. This material summarizes some of the main ideas behind **Transfer Learning** and you can also check the NIPS [presentation](https://www.youtube.com/watch?v=F1ka6a13S9I&ab_channel=LexFridman) itself.

---
### References
[1] *G. Kuhlmann and P. Stone. Graph-based domain mapping for Transfer Learning in general games. In European Conference on Machine Learning, 2007.*  
[2] *E. Eaton and M. DesJardins. Knowledge transfer with a multiresolution ensemble of classifiers. In ICML Workshop on Structural Knowledge Transfer for Machine Learning, 2006*  
[3] *E. Talvitie and S. Singh. An experts algorithm for Transfer Learning. In International Joint Conference on Artificial Intelligence, 2007.*  
[4] *C. Carroll and K. Seppi. Task similarity measures for transfer in reinforcement learning task libraries. In IEEE International Joint Conference on Neural Networks, 2005.*  
[5] *E. Eaton, M. DesJardins, and T. Lane. Modeling transfer relationships between learning tasks for improved inductive transfer. In European Conference on Machine Learning, 2008.*  
